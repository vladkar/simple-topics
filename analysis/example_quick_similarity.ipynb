{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "import spacy\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = './news.csv'\n",
    "text_col = 'title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file, header=None, names=['class', 'title', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    Fears for T N pension after talks\n",
       "1    The Race is On: Second Private Team Sets Launc...\n",
       "2        Ky. Company Wins Grant to Study Peptides (AP)\n",
       "3        Prediction Unit Helps Forecast Wildfires (AP)\n",
       "4          Calif. Aims to Limit Farm-Related Smog (AP)\n",
       "5    Open Letter Against British Copyright Indoctri...\n",
       "6                         Loosing the War on Terrorism\n",
       "7    FOAFKey: FOAF, PGP, Key Distribution, and Bloo...\n",
       "8                     E-mail scam targets police chief\n",
       "9                    Card fraud unit nets 36,000 cards\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[text_col][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    fears for t n pension after talks\n",
       "1    the race is on: second private team sets launc...\n",
       "2        ky. company wins grant to study peptides (ap)\n",
       "3        prediction unit helps forecast wildfires (ap)\n",
       "4          calif. aims to limit farm-related smog (ap)\n",
       "5    open letter against british copyright indoctri...\n",
       "6                         loosing the war on terrorism\n",
       "7    foafkey: foaf, pgp, key distribution, and bloo...\n",
       "8                     e-mail scam targets police chief\n",
       "9                    card fraud unit nets 36,000 cards\n",
       "Name: lower, dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lower'] = df[text_col].str.lower()\n",
    "df['lower'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    fears for t n pension after talks\n",
       "1    the race is on  second private team sets launc...\n",
       "2        ky  company wins grant to study peptides  ap \n",
       "3        prediction unit helps forecast wildfires  ap \n",
       "4          calif  aims to limit farm related smog  ap \n",
       "5    open letter against british copyright indoctri...\n",
       "6                         loosing the war on terrorism\n",
       "7    foafkey  foaf  pgp  key distribution  and bloo...\n",
       "8                     e mail scam targets police chief\n",
       "9                    card fraud unit nets        cards\n",
       "Name: az, dtype: object"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['az'] = df['lower'].str.replace('[^a-z]', ' ') # A-Za-z, а-я, etc...\n",
    "df['az'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['az'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [fears, for, t, n, pension, after, talks]\n",
       "1    [the, race, is, on, , second, private, team, s...\n",
       "2    [ky, , company, wins, grant, to, study, peptid...\n",
       "3    [prediction, unit, helps, forecast, wildfires,...\n",
       "4    [calif, , aims, to, limit, farm, related, smog...\n",
       "5    [open, letter, against, british, copyright, in...\n",
       "6                   [loosing, the, war, on, terrorism]\n",
       "7    [foafkey, , foaf, , pgp, , key, distribution, ...\n",
       "8              [e, mail, scam, targets, police, chief]\n",
       "9       [card, fraud, unit, nets, , , , , , , , cards]\n",
       "Name: token_1, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token_1'] = df['az'].str.split(' ')\n",
    "df['token_1'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [fears, for, t, n, pension, after, talks]\n",
       "1    [the, race, is, on, second, private, team, set...\n",
       "2    [ky, company, wins, grant, to, study, peptides...\n",
       "3    [prediction, unit, helps, forecast, wildfires,...\n",
       "4    [calif, aims, to, limit, farm, related, smog, ap]\n",
       "5    [open, letter, against, british, copyright, in...\n",
       "6                   [loosing, the, war, on, terrorism]\n",
       "7    [foafkey, foaf, pgp, key, distribution, and, b...\n",
       "8              [e, mail, scam, targets, police, chief]\n",
       "9                     [card, fraud, unit, nets, cards]\n",
       "Name: token_2, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.api import StringTokenizer\n",
    "\n",
    "df['token_2'] = df['az'].apply(nltk.word_tokenize)\n",
    "df['token_2'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter too short and too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_filter = lambda doc: [token for token in doc if len(token) >= 3 and len(token) <= 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  [fears, for, pension, after, talks]\n",
       "1    [the, race, second, private, team, sets, launc...\n",
       "2              [company, wins, grant, study, peptides]\n",
       "3       [prediction, unit, helps, forecast, wildfires]\n",
       "4            [calif, aims, limit, farm, related, smog]\n",
       "5    [open, letter, against, british, copyright, in...\n",
       "6                       [loosing, the, war, terrorism]\n",
       "7    [foafkey, foaf, pgp, key, distribution, and, b...\n",
       "8                 [mail, scam, targets, police, chief]\n",
       "9                     [card, fraud, unit, nets, cards]\n",
       "Name: token_1_len, dtype: object"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token_1_len'] = df['token_1'].apply(len_filter)\n",
    "df['token_1_len'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  [fears, for, pension, after, talks]\n",
       "1    [the, race, second, private, team, sets, launc...\n",
       "2              [company, wins, grant, study, peptides]\n",
       "3       [prediction, unit, helps, forecast, wildfires]\n",
       "4            [calif, aims, limit, farm, related, smog]\n",
       "5    [open, letter, against, british, copyright, in...\n",
       "6                       [loosing, the, war, terrorism]\n",
       "7    [foafkey, foaf, pgp, key, distribution, and, b...\n",
       "8                 [mail, scam, targets, police, chief]\n",
       "9                     [card, fraud, unit, nets, cards]\n",
       "Name: token_2_len, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token_2_len'] = df['token_2'].apply(len_filter)\n",
    "df['token_2_len'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                              [fears, pension, talks]\n",
       "1    [race, second, private, team, sets, launch, da...\n",
       "2              [company, wins, grant, study, peptides]\n",
       "3       [prediction, unit, helps, forecast, wildfires]\n",
       "4            [calif, aims, limit, farm, related, smog]\n",
       "5    [open, letter, british, copyright, indoctrinat...\n",
       "6                            [loosing, war, terrorism]\n",
       "7    [foafkey, foaf, pgp, key, distribution, bloom,...\n",
       "8                 [mail, scam, targets, police, chief]\n",
       "9                     [card, fraud, unit, nets, cards]\n",
       "Name: no_stop, dtype: object"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "print(stop)\n",
    "df['no_stop'] = df['token_1_len'].apply(lambda doc: [token for token in doc if token not in stop])\n",
    "df['no_stop'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer # simple one\n",
    "from nltk.stem.snowball import SnowballStemmer # Porter 2\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                [fear, pension, talk]\n",
       "1    [race, second, privat, team, set, launch, date...\n",
       "2                 [compani, win, grant, studi, peptid]\n",
       "3             [predict, unit, help, forecast, wildfir]\n",
       "4               [calif, aim, limit, farm, relat, smog]\n",
       "5    [open, letter, british, copyright, indoctrin, ...\n",
       "6                                  [loos, war, terror]\n",
       "7    [foafkey, foaf, pgp, key, distribut, bloom, fi...\n",
       "8                   [mail, scam, target, polic, chief]\n",
       "9                       [card, fraud, unit, net, card]\n",
       "Name: stem, dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "df['stem'] = df['no_stop'].apply(lambda doc: [stemmer.stem(token) for token in doc])\n",
    "df['stem'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                [fear, pension, talk]\n",
       "1    [race, second, private, team, set, launch, dat...\n",
       "2                [company, win, grant, study, peptide]\n",
       "3         [prediction, unit, help, forecast, wildfire]\n",
       "4             [calif, aim, limit, farm, related, smog]\n",
       "5    [open, letter, british, copyright, indoctrinat...\n",
       "6                            [loosing, war, terrorism]\n",
       "7    [foafkey, foaf, pgp, key, distribution, bloom,...\n",
       "8                  [mail, scam, target, police, chief]\n",
       "9                       [card, fraud, unit, net, card]\n",
       "Name: lem, dtype: object"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lem'] = df['no_stop'].apply(lambda doc: [lemmatizer.lemmatize(token) for token in doc])\n",
    "df['lem'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                [fear, pension, talk]\n",
       "1    [race, second, private, team, set, launch, dat...\n",
       "2                [company, win, grant, study, peptide]\n",
       "3         [prediction, unit, help, forecast, wildfire]\n",
       "4             [calif, aim, limit, farm, related, smog]\n",
       "5    [open, letter, british, copyright, indoctrinat...\n",
       "6                            [loosing, war, terrorism]\n",
       "7    [foafkey, foaf, pgp, key, distribution, bloom,...\n",
       "8                  [mail, scam, target, police, chief]\n",
       "9                       [card, fraud, unit, net, card]\n",
       "Name: lem, dtype: object"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[(df['lem'].apply(len) > 0) | (df['stem'].apply(len) > 0)]\n",
    "df['lem'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7275\n",
      "8529\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "stem_bow = vectorizer.fit_transform(df['stem'].apply(' '.join))\n",
    "stem_bow_dimensions = vectorizer.get_feature_names()\n",
    "print(len(stem_bow_dimensions))\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "lem_bow = vectorizer.fit_transform(df['lem'].apply(' '.join))\n",
    "lem_bow_dimensions = vectorizer.get_feature_names()\n",
    "print(len(lem_bow_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bow = pd.DataFrame(stem_bow.toarray(), columns=stem_bow_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency - inverted document frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7275\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "stem_tfidf = vectorizer.fit_transform(df['stem'].apply(' '.join))\n",
    "stem_tfidf_dimensions = vectorizer.get_feature_names()\n",
    "print(len(stem_bow_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(stem_tfidf.toarray(), columns=stem_tfidf_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 50\n",
    "steps = 200\n",
    "window = 4\n",
    "no_below_filter = 5\n",
    "random = 42\n",
    "d2v_model_path = './d2v_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 20:10:07.323155 16444 base_any2vec.py:723] consider setting layer size to a multiple of 4 for greater performance\n",
      "C:\\Users\\vkarbovs\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "W0820 20:11:50.853715 16444 base_any2vec.py:1386] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df['stem'])]\n",
    "model = Doc2Vec(documents, vector_size=vector_size, window=window, min_count=no_below_filter, workers=4, seed=random, steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(d2v_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(d2v_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = None\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    embeddings = embed(list(df[text_col]))\n",
    "    emb = session.run(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Find simillar texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(embeddings, metric='cosine'):\n",
    "    return squareform(pdist(embeddings, metric=metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_ind(similarity_matrix, title_id, n):\n",
    "    closest = similarity_matrix[title_id,:].argsort()[::-1][-top_n:][::-1]\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calc similarity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_use = build_similarity_matrix(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_bow = build_similarity_matrix(stem_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_tfidf = build_similarity_matrix(stem_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_d2v = build_similarity_matrix(model.docvecs.vectors_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: The Race is On: Second Private Team Sets Launch Date for Human Spaceflight (SPACE.com)\n"
     ]
    }
   ],
   "source": [
    "title_id = 5624\n",
    "title_id = 1\n",
    "\n",
    "top_n = 20\n",
    "print('Document:', df.iloc[title_id][text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_use = get_top_n_ind(sm_use, title_id, top_n)\n",
    "closest_bow = get_top_n_ind(sm_bow, title_id, top_n)\n",
    "closest_tfidf = get_top_n_ind(sm_tfidf, title_id, top_n)\n",
    "closest_d2v = get_top_n_ind(sm_d2v, title_id, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW\n",
      "1       The Race is On: Second Private Team Sets Launc...\n",
      "5455                       Launch Date Set for Solar Sail\n",
      "5453                           Solar Sail Launch Date Set\n",
      "2894    Creators of private spaceship announce plans f...\n",
      "2770             Date with destiny for private rocketeers\n",
      "1309                                               SI.com\n",
      "2925                     EU set to launch 'transit camps'\n",
      "6565                      EasyMobile launch set for March\n",
      "386                                                SI.com\n",
      "7559                      Exploring Andromeda (SPACE.com)\n",
      "2650    Virgin to Launch Commercial Space Flights by 2007\n",
      "6968           Japan to Resume Space Rocket Launches (AP)\n",
      "4782               Racing in an Evening Gown (Forbes.com)\n",
      "3739                    Next space station crew to launch\n",
      "4925                     Martin wins second straight race\n",
      "2790            Salesforce.com launches on-demand support\n",
      "7472             No Safe Place for Satellites (SPACE.com)\n",
      "749     'Dream Team' Out of Gold Race After Loss to Ar...\n",
      "5123         NASA Picks May 2005 Shuttle Launch Date (AP)\n",
      "5446             Solar spacecraft set to launch next year\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('BOW')\n",
    "print(df.iloc[closest_bow][text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF\n",
      "1       The Race is On: Second Private Team Sets Launc...\n",
      "2894    Creators of private spaceship announce plans f...\n",
      "5453                           Solar Sail Launch Date Set\n",
      "5455                       Launch Date Set for Solar Sail\n",
      "2770             Date with destiny for private rocketeers\n",
      "4925                     Martin wins second straight race\n",
      "1309                                               SI.com\n",
      "386                                                SI.com\n",
      "2842    Going Private: The Promise and Danger of Space...\n",
      "5123         NASA Picks May 2005 Shuttle Launch Date (AP)\n",
      "6376               PSG lead race for second Group H berth\n",
      "6968           Japan to Resume Space Rocket Launches (AP)\n",
      "7559                      Exploring Andromeda (SPACE.com)\n",
      "749     'Dream Team' Out of Gold Race After Loss to Ar...\n",
      "3739                    Next space station crew to launch\n",
      "2925                     EU set to launch 'transit camps'\n",
      "4307        Trial Date Set for Soldier at Abu Ghraib (AP)\n",
      "7268    Lockheed to Launch Rocket; Boeing Gets New Dat...\n",
      "7472             No Safe Place for Satellites (SPACE.com)\n",
      "7509    EU Sets Date for Turkey Talks, Demands Concess...\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('TFIDF')\n",
    "print(df.iloc[closest_tfidf][text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC2VEC\n",
      "1       The Race is On: Second Private Team Sets Launc...\n",
      "2764    .Mac bumps up storage capacity, improves mail ...\n",
      "2379    BLOG That #39;s the most look-ed up world on M...\n",
      "1143         AT amp;T Wireless Moves to Sell Canada Asset\n",
      "4577    Karzai happy to wait for official Afghan poll ...\n",
      "600           Belarus Bank Denies Money Laundering Charge\n",
      "1450    Bank sits tight on rates as house price inflat...\n",
      "2548                     Jet lands in UK after bomb alert\n",
      "5667    Greek, British Police Break Illegal Software Ring\n",
      "5586                  Report: EADS Could Link With Thales\n",
      "7321                  Trade Gap Swells More Than Expected\n",
      "3311        At Least 37 Killed, 52 Hurt in Pakistan Blast\n",
      "2039                                    #39;Noles Rebound\n",
      "834         Schumacher Clinches Seventh Season Title (AP)\n",
      "7597                               Martinez leaves bitter\n",
      "6215                    Iran pledges to halt nuclear work\n",
      "844             Thousands Hit NYC Streets; Cheney Arrives\n",
      "2876      Kendall Gives Jets' Offensive Line a Boost (AP)\n",
      "3518      Taiwan's Leader Urges China to Begin Talks (AP)\n",
      "1167    Apache Balks At Microsoft #39;s Licensing Dema...\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('DOC2VEC')\n",
    "print(df.iloc[closest_d2v][text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE\n",
      "1       The Race is On: Second Private Team Sets Launc...\n",
      "5446             Solar spacecraft set to launch next year\n",
      "3717    New Crew Prepares for Launch to International ...\n",
      "2894    Creators of private spaceship announce plans f...\n",
      "3004    NASA puts off space shuttle flights until at l...\n",
      "4915                      Nasa to resume shuttle missions\n",
      "114     Cassini Space Probe Spots Two New Saturn Moons...\n",
      "5852    NASA delays flight of X-43A scramjet to attemp...\n",
      "5718                     NASA to test hypersonic scramjet\n",
      "5551           European spacecraft prepares to orbit Moon\n",
      "3739                    Next space station crew to launch\n",
      "3730    LIVE: Launch of Expedition Ten Crew to the ISS...\n",
      "5611        Europes First Moon Probe to Enter Lunar Orbit\n",
      "1346                  Space Capsule Heading Back to Earth\n",
      "5038                           NASA looking at May launch\n",
      "5123         NASA Picks May 2005 Shuttle Launch Date (AP)\n",
      "1762    Technical Hitch Delays Russia Space Station La...\n",
      "4546    Cassini-Huygens Fly-By at Titan / ESA TV Live ...\n",
      "5863                 European probe arrives to orbit moon\n",
      "17          Mars Rovers Relay Images Through Mars Express\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('USE')\n",
    "print(df.iloc[closest_use][text_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
